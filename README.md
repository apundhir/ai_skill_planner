# AI Skill Planner

A comprehensive platform for analyzing team skill gaps, capacity planning, and strategic workforce development in AI/ML organizations.

## ğŸ¯ Project Objective

The AI Skill Planner addresses the critical challenge of skill gap identification and capacity planning in rapidly evolving AI/ML organizations. As technology advances and project requirements become increasingly complex, organizations need sophisticated tools to:

- **Identify skill gaps** before they become project bottlenecks
- **Optimize resource allocation** across multiple AI/ML projects
- **Plan strategic workforce development** initiatives
- **Minimize project delays** caused by skill shortages
- **Make data-driven hiring and training decisions**

## ğŸš€ Key Capabilities

### ğŸ“Š Executive Dashboard
- **Role-based access control** with tailored views for Admin, Executive, Manager, and Analyst roles
- **Real-time skill gap visualization** with interactive heat maps
- **Financial impact analysis** including NPV calculations and cost-of-delay modeling
- **Risk assessment** with Monte Carlo simulations for project outcomes
- **Automated recommendations** for hiring, training, and resource allocation

### ğŸ” Advanced Analytics
- **Gap Analysis Engine**: Identifies skill shortages across projects and phases
- **Proficiency Calculator**: Tracks skill decay and updates effective skill levels
- **Capacity Model**: Analyzes team utilization and availability
- **Financial Model**: Calculates business impact of skill gaps and interventions
- **Validation Framework**: Ensures model accuracy with ground truth tracking

### ğŸ¢ Production-Ready Features
- **Excel File Upload**: Bulk data import with comprehensive validation
- **Project Onboarding**: 4-step wizard for new project creation
- **User Management**: Role-based project assignments and access control
- **Automatic Recalculation**: Background processing for metrics updates
- **API Security**: JWT authentication with permission-based endpoints

### ğŸ“ˆ Business Intelligence
- **Heat Map Visualization**: Interactive skill gap matrix across projects
- **Top Gaps Analysis**: Prioritized list of critical skill shortages
- **Organization Overview**: High-level metrics and KPIs
- **Evidence Tracking**: Skills validation and certification management
- **Assignment Optimization**: Smart resource allocation recommendations

## ğŸ›  Technology Stack

### Backend
- **FastAPI**: Modern, high-performance web framework
- **SQLite**: Embedded database with full SQL support
- **Pydantic**: Data validation and settings management
- **Pandas**: Data manipulation and analysis
- **NumPy**: Numerical computing and Monte Carlo simulations

### Frontend
- **Vanilla JavaScript**: No framework dependencies for maximum performance
- **Chart.js**: Interactive charts and visualizations
- **D3.js**: Advanced data visualization for heat maps
- **CSS Grid/Flexbox**: Responsive, modern UI design

### Infrastructure
- **Docker**: Containerized deployment
- **Uvicorn**: ASGI server for production
- **Conda**: Environment management
- **Make**: Build automation and task management

## ğŸ“ Project Structure

```
ai_skill_planner/
â”œâ”€â”€ api/                          # FastAPI application
â”‚   â”œâ”€â”€ main.py                   # Main application entry point
â”‚   â”œâ”€â”€ admin_endpoints.py        # Admin-only endpoints (data upload, user management)
â”‚   â”œâ”€â”€ demo_auth.py             # Simplified authentication for demo
â”‚   â”œâ”€â”€ executive_endpoints.py   # Executive dashboard endpoints
â”‚   â”œâ”€â”€ gap_endpoints.py         # Skill gap analysis endpoints
â”‚   â”œâ”€â”€ metrics_recalculation.py # Automatic metrics updating
â”‚   â”œâ”€â”€ user_project_endpoints.py # User-project assignment management
â”‚   â””â”€â”€ validation_endpoints.py  # Model validation endpoints
â”œâ”€â”€ database/                     # Database management
â”‚   â”œâ”€â”€ init_db.py               # Database initialization
â”‚   â”œâ”€â”€ schema.sql               # Database schema definition
â”‚   â””â”€â”€ ai_skill_planner.db      # SQLite database file
â”œâ”€â”€ engines/                      # Core analytics engines
â”‚   â”œâ”€â”€ gap_analysis.py          # Skill gap detection and analysis
â”‚   â”œâ”€â”€ proficiency.py           # Skill level calculation with decay
â”‚   â”œâ”€â”€ capacity.py              # Team capacity and utilization analysis
â”‚   â”œâ”€â”€ financial.py             # NPV and financial impact modeling
â”‚   â”œâ”€â”€ risk_assessment.py       # Monte Carlo risk analysis
â”‚   â”œâ”€â”€ recommendations.py       # AI-powered recommendations
â”‚   â””â”€â”€ validation.py            # Model accuracy validation
â”œâ”€â”€ data/                         # Sample data generation
â”‚   â”œâ”€â”€ skills_taxonomy.py       # AI/ML skills taxonomy
â”‚   â”œâ”€â”€ generate_people.py       # Sample team data
â”‚   â”œâ”€â”€ generate_projects.py     # Sample project data
â”‚   â””â”€â”€ generate_assignments.py  # Sample assignments
â”œâ”€â”€ static/                       # Frontend assets
â”‚   â”œâ”€â”€ executive_dashboard.html  # Main dashboard interface
â”‚   â””â”€â”€ heatmap.html             # Skill gap heat map
â”œâ”€â”€ security/                     # Security and authentication
â”‚   â”œâ”€â”€ auth.py                  # JWT authentication system
â”‚   â””â”€â”€ encryption.py           # Data encryption utilities
â”œâ”€â”€ scripts/                      # Deployment and setup
â”‚   â””â”€â”€ setup_environment.sh    # Environment setup script
â”œâ”€â”€ docker-compose.yml           # Docker orchestration
â”œâ”€â”€ Dockerfile                   # Container configuration
â”œâ”€â”€ Makefile                     # Build and deployment tasks
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ environment.yml              # Conda environment
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Docker (optional)
- Make (optional)

### Option 1: Local Development

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ai_skill_planner
   ```

2. **Set up environment**
   ```bash
   # Using conda (recommended)
   conda env create -f environment.yml
   conda activate ai_skill_planner

   # Or using pip
   pip install -r requirements.txt
   ```

3. **Initialize database**
   ```bash
   python database/init_db.py
   ```

4. **Generate sample data**
   ```bash
   python data/skills_taxonomy.py
   python data/generate_people.py
   python data/generate_projects.py
   python data/generate_assignments.py
   python data/generate_evidence.py
   ```

5. **Start the application**
   ```bash
   python -m uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
   ```

### Option 2: Docker Deployment

1. **Using Docker Compose**
   ```bash
   docker-compose up --build
   ```

## ğŸ“œ Logging & Observability

Structured logging uses Python's built-in logging module with a JSON formatter so events can be shipped to log aggregation
platforms. Each API request is wrapped in middleware that captures the request path, method,
client address, status code, latency, and a generated `request_id`. The `request_id` is returned in the `X-Request-ID` response
header so downstream services can correlate entries.

### Configuration

Logging behaviour is controlled through environment variables and can be set in `.env` or the deployment environment:

| Variable | Description | Default |
| --- | --- | --- |
| `LOG_LEVEL` | Minimum log level (`DEBUG`, `INFO`, `WARNING`, etc.) | `INFO` |
| `LOG_SINK` | Where to write logs. Accepts `stdout`, `stderr`, or a file path. | `stdout` |
| `LOG_JSON_INDENT` | Optional JSON indentation (integer) for human-readable output. | _None_ |
| `LOG_TIMESTAMP_FORMAT` | Timestamp format for log entries (`iso`, `%Y-%m-%dT%H:%M:%S`, etc.). | `iso` |

Example `.env` snippet for verbose local debugging:

```env
LOG_LEVEL=DEBUG
LOG_SINK=stdout
LOG_JSON_INDENT=2
```

Modules should obtain a logger via `from api.core.logging import get_logger` and emit events with contextual key-value pairs:

```python
from api.core.logging import get_logger

logger = get_logger(__name__)
logger.info("metrics_refresh_started", project_id=project_id)
```

## âœ… Running Tests

The project uses `pytest` for unit and integration coverage. Test dependencies are included in both `requirements.txt` and `environment.yml`, so they are installed automatically during the setup commands above.

1. **Activate your environment** (conda or virtualenv):
   ```bash
   conda activate ai_skill_planner
   # or
   source venv/bin/activate
   ```

2. **Run the full test suite** using the Makefile target:
   ```bash
   make test
   ```
   This runs `pytest` with sensible defaults (`--maxfail=1 --disable-warnings`) and uses an isolated SQLite database configured by the fixtures in `tests/conftest.py`.

3. **Run pytest directly** for custom options such as verbose output or coverage:
   ```bash
   pytest -vv
   pytest --cov=api --cov=database
   ```

4. **Lint before pushing** to match the CI pipeline:
   ```bash
   make lint
   ```

The GitHub Actions workflow (`.github/workflows/tests.yml`) executes `make lint` and `make test` on every push and pull request, so running those commands locally keeps results consistent with CI.

2. **Using Make (if available)**
   ```bash
   make run
   ```

### Access the Application

Open your browser and navigate to: [http://localhost:8000](http://localhost:8000)

## âš™ï¸ Configuration

Runtime configuration is centralised in [`api/core/config.py`](api/core/config.py) and powered by Pydantic's `BaseSettings`. The
settings loader reads environment variables (and optional `.env` files) to control database connections, authentication
behaviour, CORS, and feature flags.

| Environment Variable | Description | Default |
|----------------------|-------------|---------|
| `APP_ENV` | Current deployment environment. | `development` |
| `DATABASE_URL` | SQLAlchemy-style database URL. Supports SQLite, PostgreSQL, etc. | `sqlite:///â€¦/database/ai_skill_planner.db` |
| `CORS_ALLOWED_ORIGINS` | Comma-separated list of allowed origins for the API. | `http://localhost:3000,http://127.0.0.1:3000` in development |
| `JWT_SECRET_KEY` | Symmetric key for signing JWT tokens. Required outside development. | _unset_ |
| `JWT_ALGORITHM` | JWT signing algorithm. | `HS256` |
| `ACCESS_TOKEN_EXPIRE_MINUTES` | Access token lifetime in minutes. | `60` |
| `REFRESH_TOKEN_EXPIRE_MINUTES` | Refresh token lifetime in minutes. | `4320` (3 days) |
| `FEATURE_ENABLE_DEMO_DATA` | Enable generation of sample/demo datasets. | `false` |
| `FEATURE_ENABLE_GAP_ANALYSIS` | Toggle gap analysis endpoints and calculations. | `true` |
| `FEATURE_ENABLE_CAPACITY_PLANNING` | Toggle capacity planning features. | `true` |

> **Legacy compatibility**: if `DATABASE_URL` is not provided, `AI_SKILL_PLANNER_DB_PATH` (a plain filesystem path) will be used
> to construct a SQLite URL.

### Production example

Create a `.env` file alongside the project root for reproducible configuration:

```env
APP_ENV=production
DATABASE_URL=postgresql+psycopg2://planner:super-secret-password@db.example.com/ai_skill_planner
JWT_SECRET_KEY=${JWT_SECRET_KEY_FROM_SECRETS_MANAGER}
CORS_ALLOWED_ORIGINS=https://planner.example.com
ACCESS_TOKEN_EXPIRE_MINUTES=30
REFRESH_TOKEN_EXPIRE_MINUTES=4320
FEATURE_ENABLE_DEMO_DATA=false
FEATURE_ENABLE_GAP_ANALYSIS=true
FEATURE_ENABLE_CAPACITY_PLANNING=true
```

Whenever you need the strongly-typed configuration inside code, import and call the cached loader:

```python
from api.core.config import get_config
from database.init_db import init_database

config = get_config()
connection = init_database(config.database_url)
connection.close()
```

All database helpers (`database/init_db.py`, `get_db_connection`, etc.) honour `DATABASE_URL`, so switching to a different
database backend requires no code changes.

## ğŸ” Demo Authentication

The system includes demo authentication with the following users:

| Role | Username | Password | Access Level |
|------|----------|----------|--------------|
| System Administrator | `admin` | `admin123` | Full system access, user management, data upload |
| Chief Technology Officer | `executive` | `exec123` | All projects, executive dashboards |
| Engineering Manager | `manager` | `manager123` | Assigned projects only |
| Data Analyst | `analyst` | `analyst123` | Assigned projects only |

## ğŸ“Š Key Features Walkthrough

### 1. Role-Based Dashboard
- Different navigation and functionality based on user role
- Executives see organization-wide metrics
- Managers see only their assigned projects
- Analysts have read-only access to relevant data

### 2. Skill Gap Heat Map
- Interactive matrix showing skill gaps across projects
- Color-coded severity levels (Critical, High, Medium, Low)
- Click-through for detailed gap analysis
- Export capabilities for further analysis

### 3. Admin Panel
- **Data Upload**: Excel file import with validation
- **Project Onboarding**: Guided wizard for new projects
- **User Management**: Assign users to projects
- **System Monitoring**: View system metrics and activity

### 4. Financial Impact Analysis
- **NPV Calculations**: Net present value of training vs. hiring
- **Cost of Delay**: Impact of skill gaps on project timelines
- **ROI Analysis**: Return on investment for skill development
- **Monte Carlo Simulations**: Risk assessment for different scenarios

### 5. Automated Recommendations
- **Hiring Recommendations**: When to hire vs. train
- **Training Priorities**: Which skills to develop first
- **Resource Allocation**: Optimal team assignments
- **Risk Mitigation**: Strategies to reduce project risk

## ğŸ”§ API Documentation

### Authentication Endpoints
- `POST /auth/login` - User authentication
- `POST /auth/logout` - User logout
- `GET /auth/me` - Current user information

### Core Data Endpoints
- `GET /skills` - List all skills with optional filtering
- `GET /people` - List team members with skill profiles
- `GET /projects` - List projects with role-based filtering
- `GET /assignments` - Project assignments and allocations

### Analytics Endpoints
- `GET /gap-analysis/organization/overview` - Organization-wide gap analysis
- `GET /gap-analysis/project/{id}/gaps` - Project-specific gaps
- `GET /gap-analysis/heatmap` - Heat map data
- `GET /gap-analysis/top-gaps` - Prioritized gap list

### Executive Endpoints
- `GET /executive/dashboard` - Executive summary metrics
- `GET /executive/financial/overview` - Financial impact analysis
- `GET /executive/recommendations/organization` - Strategic recommendations
- `GET /executive/risk/organization` - Risk assessment summary

### Admin Endpoints
- `POST /admin/upload/project` - Excel project data upload
- `POST /admin/upload/team` - Excel team data upload
- `POST /admin/onboard/project` - New project onboarding
- `GET /admin/system/metrics` - System performance metrics

### User-Project Management
- `GET /user-projects/user/{user_id}/projects` - Projects assigned to user
- `POST /user-projects/assign` - Assign user to project
- `GET /user-projects/assignments/summary` - Assignment overview

### Metrics Recalculation
- `GET /metrics/recalculation/status` - Current recalculation status
- `POST /metrics/recalculation/trigger` - Manually trigger recalculation
- `POST /metrics/demo/simulate-changes` - Simulate data changes

## ğŸ§ª Data Model

### Core Entities
- **Skills**: AI/ML skills taxonomy with decay rates
- **People**: Team members with skill profiles and availability
- **Projects**: AI/ML projects with phases and requirements
- **Assignments**: Resource allocation across projects
- **Evidence**: Skills validation and certification tracking

### Key Relationships
- People have Skills with proficiency levels
- Projects require Skills at specific levels
- People are Assigned to Projects for specific phases
- Evidence validates Skills for People

## ğŸ“ˆ Analytics Models

### Gap Analysis
- **Coverage Ratio**: Available skills vs. required skills
- **Bus Factor**: Risk of key person dependencies
- **Severity Classification**: Critical, High, Medium, Low gaps
- **Cost Impact**: Weekly cost of delays due to gaps

### Proficiency Calculation
- **Base Level**: Initial skill assessment (0-5 scale)
- **Effective Level**: Adjusted for recency and evidence
- **Confidence Intervals**: Uncertainty bounds on assessments
- **Decay Modeling**: Time-based skill degradation

### Financial Modeling
- **NPV Analysis**: Training vs. hiring cost comparison
- **Break-even Analysis**: Time to positive ROI
- **Monte Carlo Risk**: Probabilistic outcome modeling
- **Sensitivity Analysis**: Impact of key assumptions

## ğŸ“Š Sample Data Included

The system comes pre-loaded with realistic test data:

- **42 AI/ML skills** across 8 categories (Machine Learning, MLOps, Data Engineering, etc.)
- **20 team members** with diverse experience levels and hourly rates ($65-180/hr)
- **4 AI projects** with realistic timelines and financial impact
- **95+ project assignments** with capacity-aware allocation
- **270+ evidence pieces** (certifications, deployments, incidents)

### Sample Projects
1. **Real-time Fraud Detection System V2** - High-value ML project
2. **E-commerce Personalization Engine** - Customer-facing AI system
3. **Manufacturing Quality Control Vision System** - Computer vision application
4. **AI-Powered Customer Support Platform** - NLP-based automation

## ğŸš€ Deployment

### Production Considerations
- **Database**: Replace SQLite with PostgreSQL for production
- **Authentication**: Integrate with corporate SSO (LDAP/SAML)
- **Monitoring**: Add logging, metrics, and alerting
- **Security**: Enable HTTPS and security headers
- **Scaling**: Use multiple workers and load balancing

### Environment Variables
```bash
# Database
DATABASE_URL=postgresql://user:pass@localhost/dbname

# Authentication
JWT_SECRET_KEY=your-secret-key-here
JWT_ALGORITHM=HS256

# API
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=false
```

### Docker Production Build
```bash
# Build optimized production image
docker build -t ai-skill-planner:production .

# Run with environment variables
docker run -p 8000:8000 \
  -e DATABASE_URL=postgresql://... \
  -e JWT_SECRET_KEY=... \
  ai-skill-planner:production
```

## ğŸ§ª Testing

### API Health Check
```bash
# Check if API is running
curl http://localhost:8000/health

# Test authentication
curl -X POST http://localhost:8000/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"admin123"}'
```

### Interactive API Documentation
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## ğŸ”§ Development

### Adding New Features
1. **Backend**: Add endpoints in `api/` directory
2. **Analytics**: Implement engines in `engines/` directory
3. **Frontend**: Update `static/executive_dashboard.html`
4. **Database**: Modify schema in `database/schema.sql`

### Data Generation
```bash
# Regenerate sample data
python data/skills_taxonomy.py
python data/generate_people.py
python data/generate_projects.py
python data/generate_assignments.py
python data/generate_evidence.py
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For questions, issues, or feature requests:
- Open an issue on GitHub
- Check the API documentation at `/docs` when running the application
- Review the sample data generators for understanding the data model

## ğŸ‰ Acknowledgments

- Built with modern web technologies for scalability and performance
- Designed for AI/ML organizations with complex skill requirements
- Validated with real-world use cases and industry feedback

---

**AI Skill Planner** - Empowering organizations to build the right teams for AI success.
